global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'critical'
      continue: true
    - match:
        severity: warning
      receiver: 'warning'

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://backend:3001/api/v1/alerts/webhook'
        send_resolved: true

  - name: 'critical'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '${SLACK_CRITICAL_CHANNEL}'
        title: 'üö® Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
    email_configs:
      - to: '${ALERT_EMAIL}'
        from: '${ALERT_FROM_EMAIL}'
        smarthost: '${ALERT_SMTP_HOST}:${ALERT_SMTP_PORT}'
        auth_username: '${ALERT_SMTP_USER}'
        auth_password: '${ALERT_SMTP_PASSWORD}'
        headers:
          Subject: 'üö® Takumi Critical Alert: {{ .GroupLabels.alertname }}'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_INTEGRATION_KEY}'
        severity: 'critical'
        description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        client: 'Takumi Monitoring'
        client_url: 'https://grafana.takumi.example'
        details:
          alertname: '{{ .GroupLabels.alertname }}'
          severity: '{{ .CommonLabels.severity }}'
          category: '{{ .CommonLabels.category }}'

  - name: 'warning'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '${SLACK_WARNING_CHANNEL}'
        title: '‚ö†Ô∏è Warning Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
